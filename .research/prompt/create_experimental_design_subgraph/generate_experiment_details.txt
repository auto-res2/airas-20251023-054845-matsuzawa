
LLM Name: o3-2025-04-16
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "BOIL relies on a plain linear–regression cost model c(x,t)=wᵀ[x,t]+b.  In practice the true cost (wall-clock seconds) often grows non-linearly with both training-iterations t and hyper-parameters x (e.g. network width, batch-size).  A mis-specified cost model biases the cost-aware acquisition function log(EI)−log(ĉ) and can waste budget.  The limitation is therefore the overly simplistic cost model, which can be fixed with a tiny change to the algorithm.",
    "Methods": "We propose Quadratic-Cost BOIL (Q-BOIL).  Keep the whole BOIL pipeline unchanged except for the cost model:  \n1. Replace the linear regressor by a degree-2 Ridge-regularised polynomial model  ĉ(x,t)=βᵀϕ(x,t) where ϕ contains all first-order and quadratic terms (xi, t, xi·xj, xi·t, t²).  \n2. Fit β by ordinary least squares with ℓ2 regularisation (λ=1e−4 by default).  \n3. Use the new predicted mean-cost in the same acquisition function:  a(x,t)=log(EI(x,t))−log(max(ĉ(x,t),ϵ)).  \nThe change is minimal (≈5 lines of code) but allows the cost model to capture basic curvature and pairwise interactions, improving the accuracy of cost-aware selection without adding a second GP or heavy computation.",
    "Experimental Setup": "Datasets/tasks:  \n• CIFAR-10 CNN (same as original paper).  \n• CartPole-v0 DQN (fast RL task).  \nFor each task tune 4 hyper-parameters (learning rate, batch-size, γ, dropout).  Budget: 40 BO iterations.  \nBaselines:  BOIL (linear cost) vs Q-BOIL (quadratic cost).  \nMetrics:  (i) best validation score reached within a wall-clock budget; (ii) area under best-score-vs-time curve (AUC).  \nEach experiment is repeated with 10 random seeds.",
    "Experimental Code": "from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\n\nclass QBOIL(BOIL):\n    \"\"\"BOIL with a quadratic cost model\"\"\"\n    def __init__(self, func, SearchSpace, acq_name='ei_mu_max', verbose=1, poly_degree=2, l2=1e-4):\n        super().__init__(func, SearchSpace, acq_name, verbose)\n        self.cost_model = make_pipeline(PolynomialFeatures(degree=poly_degree, include_bias=False),\n                                        Ridge(alpha=l2))\n\n    # override only the part that fits/predicts cost --------------------------\n    def fit_cost_model(self):\n        XY = np.hstack((self.X, self.T))   # scaled space\n        self.cost_model.fit(XY, self.Y_cost)\n\n    def predict_cost(self, x):\n        # x is in scaled space with shape (..., d)\n        return self.cost_model.predict(x)\n\n    # small patch inside suggest_nextpoint ------------------------------------\n    def suggest_nextpoint(self):\n        # ----- same as parent up to cost fitting\n        self.gp = ProductGaussianProcess(self.scaleSearchSpace, self.gp.hyper, self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T, self.Y, self.Y_curves)\n        self.fit_cost_model()                    # <-- changed line\n        # acquisition uses self.predict_cost instead of linear regression\n        # we monkey-patch the cost evaluation used inside utility_cost_evaluation\n        self.linear_regression = None\n        return super().suggest_nextpoint()\n",
    "Expected Result": "Across all seeds Q-BOIL is expected to reach the same validation accuracy 10–20 % faster (wall-clock) than BOIL and to yield a 5–10 % higher AUC.  The number of wasted (high-cost, low-utility) trials should decrease by ≈15 %.",
    "Expected Conclusion": "A tiny replacement of the linear cost model by a quadratic Ridge regressor lets BOIL capture the first non-linear effects of hyper-parameters on runtime.  Because the rest of the pipeline is untouched, implementation effort is minimal, yet the optimiser spends its budget more judiciously, demonstrating that even very small modelling improvements in the cost surrogate translate to tangible end-to-end gains in cost-aware Bayesian optimisation."
}

# MODEL LIST
{
    "Large Language Models": {
        "Llama-4-Scout-17B-16E": {
            "model_parameters": {
                "total_parameters": "109b",
                "active_parameters": "17b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "Llama-4-Maverick-17B-128E": {
            "model_parameters": {
                "total_parameters": "400b",
                "active_parameters": "17b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "Qwen3-0.6B": {
            "model_parameters": "0.6b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-1.7B": {
            "model_parameters": "1.7b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-1.7B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-4B": {
            "model_parameters": "4b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-4B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-8B": {
            "model_parameters": "8b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-8B",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-14B": {
            "model_parameters": "14b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-14B",
            "language_distribution": "",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "Qwen3-32B": {
            "model_parameters": "32.8b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/Qwen/Qwen3-32B",
            "language_distribution": "",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers>=4.51.0"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
            "citation": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report},\n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388},\n}"
        },
        "DeepSeek-v3": {
            "model_parameters": {
                "total_parameters": "671b",
                "active_parameters": "37b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report},\n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437},\n}"
        },
        "DeepSeek-V3.1": {
            "model_parameters": {
                "total_parameters": "671B",
                "active_parameters": "37B"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "Text"
            ],
            "output_modalities": [
                "Text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report},\n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437},\n}"
        },
        "DeepSeek-V3.2-Exp": {
            "model_parameters": {
                "total_parameters": "671B",
                "active_parameters": "37B"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "Text"
            ],
            "output_modalities": [
                "Text"
            ],
            "dependent packages": [],
            "code": "",
            "citation": "@misc{deepseekai2024deepseekv32,\n      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\n      author={DeepSeek-AI},\n      year={2025},\n}"
        },
        "gpt-oss-20b": {
            "model_parameters": {
                "total_parameters": "21b",
                "active_parameters": "3.6b"
            },
            "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/openai/gpt-oss-20b",
            "context_length": "",
            "language_distribution": "multilingual",
            "input_modalities": "text",
            "output_modalities": "text",
            "dependent packages": [
                "accelerate",
                "transformers",
                "kernels"
            ],
            "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\nmessages = [\n    {\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card},\n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925},\n}"
        },
        "gemma-3-1b-it": {
            "model_parameters": "1b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-1b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        },
        "gemma-3-4b-it": {
            "model_parameters": "4b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-4b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        },
        "gemma-3-27b-it": {
            "model_parameters": "27b",
            "model_architecture": "Transformer",
            "training_data_sources": "",
            "huggingface_url": "https://huggingface.co/google/gemma-3-27b-it",
            "language_distribution": "Multilingual",
            "input_modalities": [
                "text",
                "image"
            ],
            "output_modalities": [
                "text"
            ],
            "dependent packages": [
                "transformers"
            ],
            "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\n\tmessages,\n\tadd_generation_prompt=True,\n\ttokenize=True,\n\treturn_dict=True,\n\treturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
            "citation": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
        }
    },
    "Vision Language Models": {},
    "Vision Language Action Models": {},
    "Diffusion Models": {}
}

# DATASET LIST
{
    "Text Datasets": {
        "alpaca-cleaned": {
            "discription": "",
            "num_training_samples": "",
            "num_validation_samples": "",
            "huggingface_url": "https://huggingface.co/datasets/yahma/alpaca-cleaned",
            "language_distribution": "",
            "dependent packages": [],
            "code": "",
            "citation": ""
        },
        "databricks-dolly-15k": "",
        "gsm8k": {
            "discription": "A dataset of elementary school math word problems requiring 2 to 8 steps to solve",
            "num_training_samples": 7473,
            "num_validation_samples": 1319,
            "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
            "language_distribution": "English",
            "dependent packages": [],
            "code": "",
            "citation": "@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}"
        },
        "MATH": {
            "discription": "The MATH dataset consists of approximately 12,500 mathematics problems ranging from middle school to early university level. Each problem includes a natural language question, a detailed step-by-step solution, and a final answer, and it is widely used to evaluate large language models (LLMs) in terms of their abilities in mathematical reasoning, logical inference, and step-by-step problem solving.",
            "num_training_samples": 12500,
            "num_validation_samples": 0,
            "huggingface_url": "https://huggingface.co/datasets/qwedsacf/competition_math",
            "language_distribution": "English",
            "dependent packages": [],
            "code": "",
            "example": "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n 'level': 'Level 1',\n 'type': 'Counting & Probability',\n 'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\frac{5}{12}+\\frac{1}{3}+x$, from which we have $x=\\boxed{\\frac{1}{4}}$.'}",
            "data_structure": "A data instance consists of a competition math problem and its step-by-step solution written in LaTeX and natural language. The step-by-step solution contains the final answer enclosed in LaTeX's \boxed tag.\n- problem: The competition math problem.\n- solution: The step-by-step solution.\n- level: The problem's difficulty level from 'Level 1' to 'Level 5', where a subject's easiest problems for humans are assigned to 'Level 1' and a subject's hardest problems are assigned to 'Level 5'.\n- type: The subject of the problem: Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra and Precalculus.",
            "citation": "@article{hendrycksmath2021,\n    title={Measuring Mathematical Problem Solving With the MATH Dataset},\n    author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n    journal={arXiv preprint arXiv:2103.03874},\n    year={2021}\n}"
        }
    },
    "Image Datasets": {
        "ImageNet": "",
        "CIFAR-10": ""
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- proposed_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - Output a list of objects, where each object contains "name" (hyperparameter name) and "range" (search range).
  - For example: [{"name": "learning_rate", "range": "0.001-0.01"}, {"name": "batch_size", "range": "16,32,64"}, {"name": "weight_decay", "range": "0.0001-0.001"}]
  - Search ranges can be expressed as ranges (e.g., "0.001-0.01") or discrete values (e.g., "16,32,64").
Output:
{
    "experiment_summary": "Purpose: demonstrate that replacing the linear runtime surrogate inside BOIL with a quadratic ridge-regularised polynomial (Q-BOIL) yields better cost-aware hyper-parameter optimisation.\nWorkflow:\n1. Task: CIFAR-10 image classification with a small CNN identical to the original BOIL paper.\n2. Search-space: 4 hyper-parameters – learning_rate, batch_size, gamma (optimizer momentum/discount), dropout.\n3. Optimisers:\n   • Proposed Q-BOIL – same GP-based EI acquisition as BOIL but cost surrogate ĉ(x,t)=βᵀϕ(x,t) with all first-order and pairwise quadratic terms, ridge λ=1e-4.\n   • Baseline BOIL – original linear cost model.\n4. At each BO iteration, both methods fit their cost model on observed (x,t,cost) triples, compute acquisition a(x,t)=log(EI)−log(ĉ), suggest the next configuration, run training until t and record validation accuracy and wall-clock cost.\n5. Budget: 40 BO iterations, 10 random seeds. Hardware: 1×A100 (plenty for CIFAR-10); experiments run in parallel across the 8 GPUs.\n6. Evaluation: (i) best validation accuracy achieved within a fixed wall-clock budget, (ii) area under the best-accuracy-vs-time curve (AUC). Lower cost implies higher AUC and faster convergence.\n7. Analysis: report mean±std over seeds, plot accuracy-vs-time curves, and count “wasted” trials (high cost, < median accuracy).\nImplementation: ~5 added lines using sklearn.make_pipeline(PolynomialFeatures(degree=2), Ridge(alpha=1e-4)). All other BOIL code unchanged.",
    "evaluation_metrics": [
        "Best Validation Accuracy@Time",
        "AUC of Accuracy-vs-Time Curve"
    ],
    "models_to_use": [],
    "datasets_to_use": [
        "CIFAR-10"
    ],
    "proposed_method": "Quadratic-Cost BOIL (Q-BOIL)\nObjective: improve BOIL’s cost-aware acquisition by modelling non-linear runtime effects.\nTheory: Replace linear cost surrogate ĉ(x,t)=wᵀ[x,t]+b with degree-2 polynomial ridge model: ϕ(x,t) contains {xi, t, xi·xj, xi·t, t²}. Fit β by ridge-regularised least squares (λ=1e-4). Prediction: ĉ(x,t)=βᵀϕ(x,t).\nAlgorithm:\n1. Initialise with 5 random trials; record (x, t, validation-score, wall-clock-cost).\n2. Fit Gaussian-process performance model (unchanged).\n3. Fit quadratic ridge cost model on (x,t) → cost.\n4. Acquisition: a(x,t)=log(EI_GP(x,t)) − log(max(ĉ(x,t), ε)).\n5. Optimise a(x,t) to propose next (x,t); run training; append observations.\n6. Repeat for 40 iterations.\nComplexity: fitting the quadratic surrogate is O(N·p²) with p=|x|+1 ≤5, negligible.\nExpected gain: more accurate cost estimates reduce selection of slow/poor candidates, cutting wall-clock to target accuracy by 10–20 %.",
    "comparative_methods": [
        "BOIL (Linear Cost Model)"
    ],
    "hyperparameters_to_search": [
        {
            "name": "learning_rate",
            "range": "1e-4-1e-1"
        },
        {
            "name": "batch_size",
            "range": "32,64,128,256"
        },
        {
            "name": "gamma",
            "range": "0.8-0.999"
        },
        {
            "name": "dropout",
            "range": "0.0-0.5"
        }
    ]
}
